{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from splinter import Browser\n",
    "import time\n",
    "import pprint\n",
    "import pandas as pd\n",
    "\n",
    "# setting up pretty print\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_browser():\n",
    "    # @NOTE: Replace the path with your actual path to the chromedriver\n",
    "    executable_path = {\"executable_path\": \"/usr/local/bin/chromedriver\"}\n",
    "    return Browser(\"chrome\", **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NASA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NASA\n",
    "def scrape_latest_article():\n",
    "    browser = init_browser()\n",
    "    \n",
    "    # setting up pretty print\n",
    "    pp = pprint.PrettyPrinter(indent=2)\n",
    "    \n",
    "     # Visit NASA\n",
    "    url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'\n",
    "    browser.visit(url)\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Scrape page into Soup\n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    \n",
    "    # results are returned as an iterable list\n",
    "    latest_news = soup.find('li', class_=\"slide\")\n",
    "    \n",
    "    # setting the variables\n",
    "    title_parag = {}\n",
    "    title_parag[\"news_title\"]= latest_news.find('div', class_= 'content_title').find('a').text\n",
    "    title_parag[\"news_p\"] = latest_news.find('div', class_= 'article_teaser_body').text\n",
    "    return title_parag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JPL\n",
    "def scrape_featured_image():\n",
    "    browser = init_browser()\n",
    "    \n",
    "    # setting up pretty print\n",
    "    pp = pprint.PrettyPrinter(indent=2)\n",
    "    \n",
    "     # Visit JPL\n",
    "    base_url = 'https://www.jpl.nasa.gov'\n",
    "    url = base_url + '/spaceimages/?search=&category=Mars'\n",
    "    browser.visit(url)\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Scrape page into Soup\n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    \n",
    "    # finding the featured image\n",
    "    featured_image = soup.find('article', class_=\"carousel_item\")[\"style\"]\n",
    "    print(featured_image)\n",
    "    \n",
    "    #  url beginning index\n",
    "    url_index = featured_image.index('url(') + 5\n",
    "    print(url_index)\n",
    "    \n",
    "    # url substring\n",
    "    url_substring = featured_image[url_index:-3]\n",
    "    featured_image_url = base_url + url_substring\n",
    "    return featured_image_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TWITTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "def scrape_latest_Mars_weather_tweet():\n",
    "    browser = init_browser()\n",
    "    \n",
    "    # setting up pretty print\n",
    "    pp = pprint.PrettyPrinter(indent=2)\n",
    "    \n",
    "     # Visit NASA\n",
    "    url = 'https://twitter.com/marswxreport?lang=en'\n",
    "    browser.visit(url)\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Scrape page into Soup\n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    \n",
    "    # results\n",
    "    latest_Mars_weather_tweet = soup.find('p', class_=\"tweet-text\").text\n",
    "    return latest_Mars_weather_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mars Facts\n",
    "def mars_facts():\n",
    "    url = \"https://space-facts.com/mars/\"\n",
    "    tables = pd.read_html(url)\n",
    "    df = tables[0]\n",
    "    \n",
    "    # converting table to html\n",
    "    html_table = df.to_html()\n",
    "    \n",
    "    # strip unwanted lines to cleanup table\n",
    "    html_table.replace('\\n', '')\n",
    "\n",
    "    return html_table.replace('\\n', '')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Hemisphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_Mars_hemisphere():\n",
    "    browser = init_browser()\n",
    "    \n",
    "    # setting up pretty print\n",
    "    pp = pprint.PrettyPrinter(indent=2)\n",
    "    \n",
    "     # Visit JPL\n",
    "    base_url = 'https://astrogeology.usgs.gov'\n",
    "    url = base_url + '/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "    browser.visit(url)\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Scrape page into Soup\n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    \n",
    "    link_divs = soup.find_all('div', class_=\"description\")\n",
    "    \n",
    "    hemisphere_image_urls = []\n",
    "    \n",
    "    # looping over the link_divs to get the url for each image\n",
    "    for div in link_divs:\n",
    "        browser.visit(base_url + div.find(\"a\")[\"href\"])\n",
    "    \n",
    "        time.sleep(1)\n",
    "    \n",
    "        # Scrape page into Soup\n",
    "        html = browser.html\n",
    "        soup = bs(html, \"html.parser\")\n",
    "        \n",
    "        # creating a dictionary and populating it with the title and image url for the 4 images\n",
    "        synopsis = {}\n",
    "        synopsis[\"title\"]= soup.find(\"h2\", class_='title').text\n",
    "        synopsis[\"img_url\"] = soup.find('div', class_='downloads').find('a')['href']\n",
    "        hemisphere_image_urls.append(synopsis)\n",
    "        \n",
    "    return hemisphere_image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comprehensive function\n",
    "def scrape_info():\n",
    "    mars_dict = {}\n",
    "    mars_dict[\"nasa\"] = scrape_latest_article()\n",
    "    mars_dict[\"jpl\"] = scrape_featured_image()\n",
    "    mars_dict[\"twitter\"] = scrape_latest_Mars_weather_tweet()\n",
    "    mars_dict[\"facts\"] = mars_facts()\n",
    "    mars_dict[\"images\"] = scrape_Mars_hemisphere()\n",
    "    \n",
    "    return mars_dict\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
